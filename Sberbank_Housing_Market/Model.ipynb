{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrojesus/anaconda/envs/py3/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "/Users/pedrojesus/anaconda/envs/py3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import re\n",
    "import scipy\n",
    "from collections import defaultdict\n",
    "\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "macro_df = pd.read_csv(\"macro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_string_to_datetime(string):\n",
    "    return pd.to_datetime(string, format='%Y-%m-%d', errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "macro_columns_to_delete = ['salary_growth',\n",
    " 'retail_trade_turnover_per_cap',\n",
    " 'retail_trade_turnover_growth',\n",
    " 'unemployment',\n",
    " 'provision_retail_space_sqm',\n",
    " 'provision_retail_space_modern_sqm',\n",
    " 'baths_share',\n",
    " 'heating_share',\n",
    " 'oil_urals',\n",
    " 'unprofitable_enterpr_share',\n",
    " 'old_education_build_share',\n",
    " 'modern_education_share',\n",
    " 'grp',\n",
    " 'grp_growth',\n",
    " 'real_dispos_income_per_cap_growth',\n",
    " 'profitable_enterpr_share',\n",
    " 'unprofitable_enterpr_share',\n",
    " 'share_own_revenues',\n",
    " 'overdue_wages_per_cap',\n",
    " 'fin_res_per_cap',\n",
    " 'marriages_per_1000_cap',\n",
    " 'divorce_rate',\n",
    " 'construction_value',\n",
    " 'invest_fixed_assets_phys',\n",
    " 'pop_migration',\n",
    " 'pop_total_inc',\n",
    " 'housing_fund_sqm',\n",
    " 'lodging_sqm_per_cap',\n",
    " 'water_pipes_share',\n",
    " 'baths_share',\n",
    " 'sewerage_share',\n",
    " 'gas_share',\n",
    " 'hot_water_share',\n",
    " 'electric_stove_share',\n",
    " 'heating_share',\n",
    " 'old_house_share',\n",
    " 'infant_mortarity_per_1000_cap',\n",
    " 'perinatal_mort_per_1000_cap',\n",
    " 'incidence_population',\n",
    " 'load_of_teachers_preschool_per_teacher',\n",
    " 'child_on_acc_pre_school',\n",
    " 'provision_doctors',\n",
    " 'power_clinics',\n",
    " 'hospital_beds_available_per_cap',\n",
    " 'hospital_bed_occupancy_per_year',\n",
    " 'provision_retail_space_sqm',\n",
    " 'provision_retail_space_modern_sqm',\n",
    " 'theaters_viewers_per_1000_cap',\n",
    " 'museum_visitis_per_100_cap',\n",
    " 'population_reg_sports_share',\n",
    " 'students_reg_sports_share',\n",
    " 'apartment_build']\n",
    "\n",
    "macro_df[\"timestamp\"] = macro_df[\"timestamp\"].apply(lambda time: convert_string_to_datetime(time))\n",
    "macro_df = macro_df.drop(macro_columns_to_delete, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_cap_2015 = macro_df[macro_df['timestamp'].apply(lambda x: x.year == 2015)]['income_per_cap'].mean()\n",
    "macro_df['income_per_cap'] = macro_df['income_per_cap'].fillna(mean_cap_2015)\n",
    "\n",
    "first_deposits_rate_value = macro_df[macro_df['deposits_rate'].notnull()].head(1)['deposits_rate'].values.tolist()[0]\n",
    "macro_2010 = macro_df[macro_df['timestamp'].apply(lambda x: x.year == 2010)]\n",
    "macro_df.loc[macro_2010.index,'deposits_rate'] = macro_2010['deposits_rate'].fillna(first_deposits_rate_value)\n",
    "\n",
    "\n",
    "last_deposits_rate_value = macro_df[macro_df['deposits_rate'].notnull()].tail(1)['deposits_rate'].values.tolist()[0]\n",
    "macro_2016 = macro_df[macro_df['timestamp'].apply(lambda x: x.year == 2016)]\n",
    "macro_df.loc[macro_2016.index,'deposits_rate'] = macro_2016['deposits_rate'].fillna(last_deposits_rate_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_null_2016 = ['apartment_fund_sqm',\n",
    " 'average_life_exp',\n",
    " 'bandwidth_sports',\n",
    " 'childbirth',\n",
    " 'employment',\n",
    " 'invest_fixed_assets',\n",
    " 'invest_fixed_capital_per_cap',\n",
    " 'labor_force',\n",
    " 'load_of_teachers_school_per_teacher',\n",
    " 'load_on_doctors',\n",
    " 'mortality',\n",
    " 'pop_natural_increase',\n",
    " 'provision_nurse',\n",
    " 'retail_trade_turnover',\n",
    " 'salary',\n",
    " 'seats_theather_rfmin_per_100000_cap',\n",
    " 'students_state_oneshift',\n",
    " 'turnover_catering_per_cap']\n",
    "\n",
    "\n",
    "year_macro = macro_df[macro_df['timestamp'].apply(lambda x: x.day == 1 and x.month == 1)]\n",
    "\n",
    "def get_predicted_value(column):\n",
    "    average_increase = year_macro[column].diff().mean()\n",
    "    previous_value = year_macro[year_macro['timestamp'].apply(lambda x: x.year == 2015)][column].tolist()[0]\n",
    "    return previous_value + average_increase\n",
    "\n",
    "for var in year_null_2016:\n",
    "    value = get_predicted_value(var)\n",
    "    macro_df[var] = macro_df[var].fillna(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['full_sq'] != 5326.000000]\n",
    "train_df = train_df[train_df['life_sq'] != 7478.0]\n",
    "train_df = train_df[train_df['state'] != 33.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_year(year):\n",
    "    if year < 1800:\n",
    "        return np.nan\n",
    "    if year > 2016:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return year\n",
    "    \n",
    "#train_df['build_year'] = train_df['build_year'].apply(correct_year, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_delete = ['max_floor','metro_min_walk',\n",
    " 'public_transport_station_min_walk',\n",
    " 'railroad_station_walk_min',\n",
    " 'metro_min_avto',\n",
    " 'metro_km_avto',\n",
    " 'railroad_station_avto_km',\n",
    " 'railroad_station_avto_min',\n",
    " 'full_all',\n",
    " 'young_all',\n",
    " 'work_all',\n",
    " 'ekder_all',\n",
    " '0_6_all',\n",
    " '7_14_all',\n",
    " '0_17_all',\n",
    " '16_29_all',\n",
    " '0_13_all',\n",
    " '0_13_male',\n",
    " '0_13_female',\n",
    " '0_17_male',\n",
    " '0_17_female',\n",
    " 'young_male',\n",
    " 'young_female',\n",
    " 'children_school',\n",
    " 'children_preschool',\n",
    " 'office_count_1000',\n",
    " 'trc_count_1000',\n",
    " 'cafe_count_1000',\n",
    " 'cafe_count_1000_na_price',\n",
    " 'cafe_count_1000_price_500',\n",
    " 'cafe_count_1000_price_1000',\n",
    " 'cafe_count_1000_price_1500',\n",
    " 'cafe_count_1000_price_2500',\n",
    " 'cafe_count_1000_price_4000',\n",
    " 'cafe_count_1000_price_high',\n",
    " 'big_church_count_1000',\n",
    " 'church_count_1000',\n",
    " 'mosque_count_1000',\n",
    " 'leisure_count_1000',\n",
    " 'sport_count_1000',\n",
    " 'market_count_1000',\n",
    " 'office_count_1500',\n",
    " 'trc_count_1500',\n",
    " 'cafe_count_1500',\n",
    " 'cafe_count_1500_na_price',\n",
    " 'cafe_count_1500_price_500',\n",
    " 'cafe_count_1500_price_1000',\n",
    " 'cafe_count_1500_price_1500',\n",
    " 'cafe_count_1500_price_2500',\n",
    " 'cafe_count_1500_price_4000',\n",
    " 'cafe_count_1500_price_high',\n",
    " 'big_church_count_1500',\n",
    " 'church_count_1500',\n",
    " 'mosque_count_1500',\n",
    " 'leisure_count_1500',\n",
    " 'sport_count_1500',\n",
    " 'market_count_1500',\n",
    " 'office_count_3000',\n",
    " 'trc_count_3000',\n",
    " 'cafe_count_3000',\n",
    " 'cafe_count_3000_na_price',\n",
    " 'cafe_count_3000_price_500',\n",
    " 'cafe_count_3000_price_1000',\n",
    " 'cafe_count_3000_price_1500',\n",
    " 'cafe_count_3000_price_2500',\n",
    " 'cafe_count_3000_price_4000',\n",
    " 'cafe_count_3000_price_high',\n",
    " 'big_church_count_3000',\n",
    " 'church_count_3000',\n",
    " 'mosque_count_3000',\n",
    " 'leisure_count_3000',\n",
    " 'sport_count_3000',\n",
    " 'market_count_3000',\n",
    " 'green_part_1000',\n",
    " 'prom_part_1000',\n",
    " 'green_part_1500',\n",
    " 'prom_part_1500',\n",
    " 'green_part_3000',\n",
    " 'prom_part_3000',\n",
    " 'cafe_sum_500_min_price_avg',\n",
    " 'cafe_sum_500_max_price_avg',\n",
    " 'cafe_sum_1000_min_price_avg',\n",
    " 'cafe_sum_1000_max_price_avg',\n",
    " 'cafe_sum_1500_min_price_avg',\n",
    " 'cafe_sum_1500_max_price_avg',\n",
    " 'cafe_sum_2000_min_price_avg',\n",
    " 'cafe_sum_2000_max_price_avg',\n",
    " 'cafe_sum_3000_min_price_avg',\n",
    " 'cafe_sum_3000_max_price_avg',\n",
    " 'cafe_sum_5000_min_price_avg',\n",
    " 'cafe_sum_5000_max_price_avg',\n",
    " 'ttk_km',\n",
    " 'bulvar_ring_km',\n",
    " 'kremlin_km',\n",
    " 'raion_build_count_with_material_info',\n",
    " 'raion_build_count_with_builddate_info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "First we have to apply all transformations to a dataset (create a pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_pandas_timestamp(df):\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].apply(lambda time: convert_string_to_datetime(time))\n",
    "\n",
    "\n",
    "def log_price(df):\n",
    "    df['price_doc'] = np.log(df['price_doc'])\n",
    "   \n",
    "    \n",
    "def create_top_floor(df):\n",
    "    df['top_floor'] = (df['max_floor'] == df['floor'])\n",
    "    \n",
    "    \n",
    "def truncate_floors(df):\n",
    "    #> 25th floor truncate\n",
    "    rows = df[df['floor'] > 25]\n",
    "    df['floor'] = df['floor'].apply(lambda x: ('%.0f' % x) + \"floor\")\n",
    "    df.loc[rows.index,'floor'] = \"25+\"\n",
    "    return df\n",
    "\n",
    "def correct_build_years(df):\n",
    "    df['build_year'] = df['build_year'].apply(correct_year, 1)\n",
    "\n",
    "\n",
    "def update_weird_kitch_values(df):\n",
    "    query = df['kitch_sq'] > df['life_sq'] + df['full_sq']\n",
    "    df.ix[query, 'kitch_sq'] = np.nan\n",
    "\n",
    "\n",
    "def drop_columns(df):\n",
    "    df.drop(columns_to_delete, 1, inplace = True)\n",
    "    \n",
    "  \n",
    "def join_macro_information(df):\n",
    "    df = df.merge(macro_df, on = 'timestamp', how = 'left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_final_columns(df):\n",
    "    df.drop(['id','timestamp'], 1, inplace = True)\n",
    "\n",
    "        \n",
    "def convert_future_categories_to_string(df):\n",
    "    #having a lot of trouble with converting int to categories, so have to make sure everything is equal\n",
    "    df['material'] = df['material'].apply(lambda x: ('%.0f' % x) + \"mat\")\n",
    "    df['state'] = df['state'].apply(lambda x: ('%.0f' % x) + \"state\")\n",
    "    #nan will be turned into a category, but that is ok, because it would be done later\n",
    "    \n",
    "    return df\n",
    "        \n",
    "def convert_to_categories(df):\n",
    "    categories = ['floor', 'material', 'state', 'product_type', 'sub_area', 'culture_objects_top_25',\n",
    "                 'thermal_power_plant_raion',\n",
    "                 'incineration_raion',\n",
    "                 'oil_chemistry_raion',\n",
    "                 'radiation_raion',\n",
    "                 'railroad_terminal_raion',\n",
    "                 'big_market_raion',\n",
    "                 'nuclear_reactor_raion',\n",
    "                 'detention_facility_raion',\n",
    "                 'water_1line',\n",
    "                 'big_road1_1line',\n",
    "                 'railroad_1line',\n",
    "                 'ecology']\n",
    "    \n",
    "    for cat_var in categories:\n",
    "        df[cat_var] = df[cat_var].astype('category').apply(str)\n",
    "\n",
    "\n",
    "def transform_input(df):\n",
    "    convert_to_pandas_timestamp(df)\n",
    "    #log_price(df)\n",
    "    create_top_floor(df)\n",
    "    df = truncate_floors(df)\n",
    "    correct_build_years(df)\n",
    "    update_weird_kitch_values(df)\n",
    "    drop_columns(df)\n",
    "    df = join_macro_information(df)\n",
    "    drop_final_columns(df)\n",
    "    df = convert_future_categories_to_string(df)\n",
    "    convert_to_categories(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()\n",
    "train_df_copy = transform_input(train_df_copy)\n",
    "log_price(train_df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of missing values\n",
    "Imputer doesn't work on single columns nor groups, so do this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_values_to_impute(df):\n",
    "    # this can only be run after the pipeline, before the imputing pipeline\n",
    "    columns = ['sub_area','metro_km_walk', 'railroad_station_walk_km', 'green_part_2000', 'prom_part_5000']\n",
    "    \n",
    "    df1 = df[columns].groupby('sub_area').mean()\n",
    "    df2 = df[['sub_area','ID_railroad_station_walk']].groupby('sub_area').ID_railroad_station_walk.apply(\n",
    "        lambda x: scipy.stats.mode(x)[0][0]) #mode is trickier to apply\n",
    "    return pd.concat([df1,df2], axis = 1)\n",
    "\n",
    "\n",
    "def update_na(row, col_name):\n",
    "    if np.isnan(row[col_name]):\n",
    "        return imputing_values.loc[row['sub_area'], col_name]\n",
    "    else:\n",
    "        return row[col_name]\n",
    "\n",
    "    \n",
    "\n",
    "def impute_cafe_values(df):\n",
    "    columns = ['cafe_avg_price_500','cafe_avg_price_1000','cafe_avg_price_1500',\n",
    "               'cafe_avg_price_2000', 'cafe_avg_price_3000', 'cafe_avg_price_5000']\n",
    "    df[columns] = df[columns].fillna(value = 0)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def impute_non_complete_areas_values(df):\n",
    "    for var in ['metro_km_walk', 'railroad_station_walk_km', 'green_part_2000', \n",
    "                'prom_part_5000', 'ID_railroad_station_walk']:\n",
    "        df[var] = df.apply(lambda x: update_na(x, var),1)\n",
    "    return df   \n",
    "\n",
    "mean_columns = ['life_sq','kitch_sq','build_year','num_room',\n",
    "     'preschool_quota', 'school_quota', 'hospital_beds_raion',\n",
    "     'build_count_block', 'build_count_wood', 'build_count_frame', 'build_count_brick',\n",
    "     'build_count_monolith', 'build_count_panel', 'build_count_foam', 'build_count_slag', 'build_count_mix',\n",
    "     'build_count_before_1920', 'build_count_1921-1945', 'build_count_1946-1970', 'build_count_1971-1995',\n",
    "     'build_count_after_1995']\n",
    "\n",
    "def impute_mean_for_certain_columns(df):       \n",
    "    df[mean_columns] = df[mean_columns].fillna(mean_values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fill_categories(df):\n",
    "    df['floor'] = df['floor'].cat.add_categories(\"unknown_floor\").fillna(\"unknown_floor\")\n",
    "    df['material'] = df['material'].cat.add_categories(\"unknown_material\").fillna(\"unknown_material\")\n",
    "    df['state'] = df['state'].cat.add_categories(\"unknown_state\").fillna(\"unknown_state\")\n",
    "    #df['product_type'] = df['product_type'].cat.add_categories(-1).fillna(-1)\n",
    "    df['sub_area'] = df['sub_area'].cat.add_categories(-1).fillna(-1)\n",
    "    df['product_type'] = df['product_type'].cat.add_categories(\"no_type\").fillna(\"no_type\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def input_missing_values(df):\n",
    "    impute_cafe_values(df)\n",
    "    impute_non_complete_areas_values(df)\n",
    "    impute_mean_for_certain_columns(df)\n",
    "    fill_categories(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputing_values = compute_values_to_impute(train_df_copy) #imputing values has to be called\n",
    "mean_values = train_df_copy[mean_columns].mean()\n",
    "input_missing_values(train_df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables\n",
    "Must encode categorical variables into vectors.\n",
    "First we have to code the categories to labels (numbers), and then encode those labels into one-hot vectors because the labels do not have an order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_variables = train_df_copy.select_dtypes(include = ['category']).columns.tolist()\n",
    "\n",
    "categories_label_encoders = defaultdict(LabelEncoder)\n",
    "categories_size = []\n",
    "\n",
    "#in order for one hot vector to work with labels that do not appear on the training set \n",
    "#we have to set the number of categories beforehand\n",
    "for cat_var in category_variables:\n",
    "    cat_list = train_df_copy[cat_var].cat.categories.tolist()\n",
    "    categories_label_encoders[cat_var].fit(cat_list) #this way it knows all classes\n",
    "    categories_size += [len(cat_list)]\n",
    "    \n",
    "one_hot_encoder = OneHotEncoder(n_values = categories_size, sparse = False) #this way i can transform values that do not appear on the training set (only on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encode the several categorical variables\n",
    "\n",
    "#with one label enconder we can only encode one variable at a time\n",
    "fit_labels = train_df_copy[category_variables].apply(lambda x: categories_label_encoders[x.name].transform(x)) \n",
    "\n",
    "#one hot enconder works with several columns at once    \n",
    "one_hot_matrix = one_hot_encoder.fit_transform(fit_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join this new one-hot matrix with the matrix (and remove the obsolete columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_one_hot_with_dataframe(data_df, one_hot_matrix):\n",
    "    \"\"\"\n",
    "    Join dataframe with the one hot matrix that includes the categorical variables\n",
    "    Also drops the categorical variables from the original dataframe\n",
    "    \"\"\"\n",
    "    one_hot_column_names = [\"one_hot\" + str(i) for i in range(0,one_hot_matrix.shape[1])]\n",
    "    one_hot_df = pd.DataFrame(one_hot_matrix, columns = one_hot_column_names)\n",
    "    smaller_data_df = data_df.drop(category_variables, 1)\n",
    "\n",
    "    return pd.concat([smaller_data_df, one_hot_df], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = join_one_hot_with_dataframe(train_df_copy, one_hot_matrix)\n",
    "train_y = train_set['price_doc']\n",
    "train_x = train_set.drop('price_doc',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_copy = test_df.copy()\n",
    "test_df_copy = transform_input(test_df_copy)\n",
    "input_missing_values(test_df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_labels_test = test_df_copy[category_variables].apply(lambda x: categories_label_encoders[x.name].transform(x))\n",
    "one_hot_test_matrix = one_hot_encoder.transform(fit_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = join_one_hot_with_dataframe(test_df_copy, one_hot_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_submission_file(test_results):\n",
    "    \"\"\"\n",
    "    Given probability output, write the submission file\n",
    "    \"\"\"\n",
    "    results_series = pd.Series(test_results)   \n",
    "    pd.DataFrame({\"id\": test_df['id'], \"price_doc\": results_series}).to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.isnull().sum()[test_x.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train\n",
    "rfc = RandomForestRegressor(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_result = rfc.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_result = rfc.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = np.exp(test_result)\n",
    "write_submission_file(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:14.3668\ttest-rmse:14.3668\n",
      "[50]\ttrain-rmse:1.1939\ttest-rmse:1.20916\n",
      "[100]\ttrain-rmse:0.378775\ttest-rmse:0.474312\n",
      "[150]\ttrain-rmse:0.326073\ttest-rmse:0.4648\n"
     ]
    }
   ],
   "source": [
    "cv_output = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
    "    verbose_eval=50, show_stdv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_boost_rounds = len(cv_output)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round= num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(test_x)\n",
    "y_predict = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = np.exp(y_predict)\n",
    "write_submission_file(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
